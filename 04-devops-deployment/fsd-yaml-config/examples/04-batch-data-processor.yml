# Example 4: Batch Data Processor
# A batch job for large-scale data processing
# Demonstrates AWS Batch configuration patterns

name: daily-analytics-etl
platform: batch
description: Daily ETL job for analytics data processing

team: data-engineering
tags:
  environment: production
  tier: data-pipeline
  cost-center: analytics
  schedule: daily

# Batch compute configuration
compute:
  vcpus: 4
  memory: 8192        # 8 GB

  # Optional GPU support
  # gpu: 1

  # Compute environment preferences
  compute_environment:
    type: FARGATE      # FARGATE or EC2
    fargate_platform_version: LATEST

# Job configuration
job:
  type: single        # single, array, or multi-node

  # Retry policy
  retry_attempts: 3
  retry_strategy:
    attempts: 3
    evaluate_on_exit:
      - on_status_reason: "Host EC2*"
        action: RETRY
      - on_reason: "CannotPullContainer*"
        action: RETRY
      - on_exit_code: "137"  # OOM killed
        action: RETRY
      - action: EXIT

  # Timeout configuration
  timeout: 7200       # 2 hours in seconds

  # Job scheduling
  scheduling:
    type: cron
    expression: "0 3 * * *"  # 3 AM UTC daily
    timezone: UTC

# Container configuration
container:
  image: ${ecr:analytics-etl}:latest
  command:
    - python
    - -m
    - analytics_etl.main
    - --date
    - ${batch:scheduled_date}

  # Mount points for ephemeral storage
  mount_points:
    - source_volume: scratch
      container_path: /scratch
      read_only: false

  volumes:
    - name: scratch
      host:
        source_path: /scratch

# Environment variables
environment:
  LOG_LEVEL: info
  SOURCE_BUCKET: raw-analytics-data
  OUTPUT_BUCKET: processed-analytics
  DATABASE_URL: ${secrets:analytics-etl/database-url}
  REDSHIFT_CLUSTER: ${ssm:/analytics/redshift-cluster}
  AWS_REGION: us-east-1
